This pipeline assumes an existing conda installation and is written as a Snakemake workflow. To install Snakemake with conda, run:

conda env create -f envs/snakemake.yaml
conda activate snakemake

This will create and activate a conda environment named snakemake. Once all the input files are ready, run the pipeline on a SLURM server environment with:

./run_pipeline.sh

After the pipeline finishes, in your output directory, view your BAM files and plaintext quantification output in workup/results/, view your binary AnnData objects (which contain your single-cell count matrices for downstream processing with scanpy) in workup/results/anndatas/, check the quality of your FASTQ files in workup/results/qc/, and view your ligation efficiency in a file ending in .ligation_efficiency.txt in workup/.

For troubleshooting, you can view the log files in workup/logs/ in your output directory.

Essentially, for most cases, all this pipeline needs is your FASTQ files supplied in the samples JSON file and the paths to your index (e.g. STAR index) that you want to map your reads against.

Other common usage notes:

To run the pipeline on a local computer (e.g., laptop), comment out or remove the --cluster-config cluster.yaml and --cluster "sbatch ..." arguments within ./run_pipeline.sh, and set the number of jobs -j <#> to the number of local processors available.
run_pipeline.sh passes any additional arguments to snakemake.
To perform a dry-run: ./run_pipeline.sh --dry-run
To force (re)execution of all rules regardless of past output: ./run_pipeline.sh --forceall
To remove intermediate files generated by the pipeline: ./run_pipeline.sh clean

Other documentation:

index_info.txt: Information about creating the indices to map sequencing reads against

Useful scripts:

scripts/python/generate_barcodes.py: Generate oligo sequences, attempting to maximize hamming distance between each sequence

